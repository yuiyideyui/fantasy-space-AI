import os
import re
import json
import uvicorn
from fastapi import FastAPI, Request
from vllm import LLM, SamplingParams

# --- ç¯å¢ƒé…ç½® ---
os.environ["VLLM_USE_V1"] = "0"

app = FastAPI()

# --- é™æ€ Prompt éƒ¨åˆ† (å…¨å±€åªå®šä¹‰ä¸€æ¬¡ï¼Œæ”¾åœ¨æœ€å‰é¢ä»¥æœ€å¤§åŒ–ç¼“å­˜å‘½ä¸­) ---
# ğŸ”‘ ä¼˜åŒ–ï¼šå®Œå…¨é™æ€çš„è§„åˆ™æ”¾æœ€å‰é¢ï¼Œæ‰€æœ‰ NPC å…±äº«è¿™éƒ¨åˆ†ç¼“å­˜
STATIC_PROMPT_PREFIX = (
    "<ï½œbegin of sentenceï½œ>"
    "### ä»»åŠ¡\nè¯·åˆ†æç°çŠ¶å¹¶ç»™å‡ºä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œå¿…é¡»ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚\n"
    "ç»“æœæ ·ä¾‹ï¼š{\"thought\": \"éœ€è¦è¡¥æ°´\", \"text\": \"æˆ‘å¾—æ‰¾ç‚¹æ°´å–\", \"actions\": [{ \"type\": \"use\", \"item_name\": \"æ°´å£¶\" }]}\n"
    "# Output Format\n"
    "**æ³¨æ„ï¼šä½ å¿…é¡»ä»…è¾“å‡ºä¸€ä¸ªåˆæ³•çš„ JSON å¯¹è±¡ã€‚** ä¸¥ç¦åœ¨ JSON å‰åæ·»åŠ ä»»ä½•æ–‡å­—è¯´æ˜ã€è§£é‡Šã€æ¢è¡Œæˆ– Markdown ä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```json ï¼‰ã€‚\n"
    "\n"
    "## JSON Structure\n"
    "{\n"
    "  \"thought\": \"string (ä½ çš„å†…å¿ƒæ´»åŠ¨å’Œå†³ç­–é€»è¾‘)\",\n"
    "  \"text\": \"string (ä½ å¯¹ç©å®¶è¯´çš„è¯)\",\n"
    "  \"experience\": \"string (ä½ çš„ç»éªŒæ€»ç»“)\",\n"
    "  \"actions\": [\n"
    "    { \"type\": \"move\", \"pos\": [number, number] },\n"
    "    { \"type\": \"use\", \"item_name\": \"string\" },\n"
    "    { \"type\": \"attack\", \"sum\": number },\n"
    "    { \"type\": \"interact\" }\n"
    "  ]\n"
    "}\n"
    "\n"
    "# Action Rules\n"
    "- **main**: è¯·æ³¨æ„é¥±é£Ÿåº¦æˆ–å«æ°´é‡ä½äº0çš„æ—¶å€™ä¼šå‡å°‘ç”Ÿå‘½å€¼ï¼Œé¥±é£Ÿåº¦å’Œå«æ°´é‡é«˜äº50çš„æ—¶å€™ä¼šå›å¤ç”Ÿå‘½å€¼ï¼Œç”Ÿå‘½å€¼0çš„æ—¶å€™ä¸å…è®¸æœ‰ä»»ä½•æ“ä½œå’Œè¯­è¨€ã€æ€è€ƒã€‚\n"
    "- **experience**: ä½ çš„ç»éªŒæ€»ç»“ï¼Œè¯·æ ¹æ®ä½ ä¹‹å‰çš„è®°å½•è¿›è¡Œæ€»ç»“ã€‚\n"
    "- **actions**: å¿…é¡»æ˜¯æ•°ç»„ä¸”ä¸èƒ½ä¸ºç©º,å¿…é¡»ç¬¦åˆä¸Šé¢çš„jsonç»“æ„ã€‚\n"
    "- **move**: `pos` å¿…é¡»åœ¨åœ°å›¾èŒƒå›´å†…ä¸”å±äºå¯è¡ŒåŒºåŸŸï¼ˆç¦æ­¢è¿›å…¥ç›®æ ‡æ¸…å•ä¸­çš„ä¸å¯ç§»åŠ¨åŒºåŸŸï¼‰ï¼Œç§»åŠ¨åçš„ä½ç½®å¯ä»¥ä¼šæœ‰1-3ä¸ªå•ä½çš„è¯¯å·®ã€‚\n"
    "- **use**: `item_name` å¿…é¡»æ˜¯ä½ å½“å‰èƒŒåŒ…é‡Œå·²æœ‰çš„ç‰©å“ï¼Œuseç§å­çš„æ—¶å€™éœ€è¦åˆ°å¯ç§æ¤åœŸåœ°åŒºåŸŸä¸Šé¢æ‰å¯ç§æ¤ã€‚useä¸èƒ½ç»™åˆ«ä»–äººä½¿ç”¨ã€‚\n"
    "- **attack**: `sum` å¿…é¡»æ˜¯æ•´æ•°ï¼Œä»£è¡¨æ”»å‡»æ¬¡æ•°ã€‚\n"
    "- **interact**: åªæœ‰å½“ä½ ä½äºå¯äº¤äº’ç‰©ä½“é™„è¿‘æ—¶æ‰èƒ½æ‰§è¡Œã€‚\n"
    "- **Constraints**: ç¦æ­¢å‡ºç°æœªå®šä¹‰çš„å­—æ®µï¼Œä¸¥ç¦è¿”å› nullã€‚\n"
)

# --- 1. æ¨¡å‹åˆå§‹åŒ– ---
print("æ­£åœ¨åŠ è½½ DeepSeek-R1-14B-AWQ æ¨¡å‹...")
llm = LLM(
    model="/home/yuiyi/models/DeepSeek-R1-14B-AWQ",
    trust_remote_code=True,
    # enable_prefix_caching=True,  # ğŸ‘ˆ æ ¸å¿ƒä¼˜åŒ–ï¼šå¼€å¯å‰ç¼€ç¼“å­˜
    max_model_len=4096,
    gpu_memory_utilization=0.85,
    enforce_eager=True,
    kv_cache_dtype="fp8"
)

# --- 2. å¢å¼ºå‹ JSON æå–å‡½æ•° ---
def extract_json(text: str):
    """
    ä¸“é—¨é€‚é… DeepSeek-R1 çš„æå–é€»è¾‘ï¼š
    1. å…ˆå‰”é™¤ <think> æ ‡ç­¾å†…å®¹ï¼Œé¿å…å¹²æ‰°ã€‚
    2. ä½¿ç”¨è´ªå©ªåŒ¹é… r"({[\s\S]*})" æŠ“å–æœ€å¤–å±‚ JSONï¼Œç¡®ä¿ actions æ•°ç»„å†…çš„åµŒå¥—èŠ±æ‹¬å·ä¸è¢«æˆªæ–­ã€‚
    """
    # ç§»é™¤æ€ç»´é“¾å†…å®¹
    clean_text = re.sub(r'<think>[\s\S]*?</think>', '', text).strip()
    
    # è´ªå©ªåŒ¹é…ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„æ‰€æœ‰å†…å®¹
    match = re.search(r"(\{[\s\S]*\})", clean_text)
    
    if match:
        json_str = match.group(1)
        # ç§»é™¤å¯èƒ½è¯¯åŠ çš„ Markdown æ ‡è¯†
        json_str = json_str.replace('```json', '').replace('```', '').strip()
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            # å°è¯•ä¿®å¤æœ«å°¾å¤šä½™é€—å·çš„å¸¸è§é”™è¯¯
            try:
                json_str = re.sub(r',\s*([\]}])', r'\1', json_str) 
                return json.loads(json_str)
            except:
                return None
    return None

# --- 3. è·¯ç”±å®šä¹‰ ---
@app.post("/generate")
async def generate(request: Request):
    try:
        body = await request.json()
        scene_report = body.get("scene_report", "")
        system_prompt = body.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æ¸¸æˆç©å®¶ã€‚")

        if not scene_report:
            return {"status": "error", "message": "ç¼ºå°‘ç¯å¢ƒæŠ¥å‘Š"}

        # ğŸ”‘ ä¼˜åŒ–åçš„ prompt ç»„è£…é¡ºåºï¼š
        # 1. å…ˆæ”¾å®Œå…¨é™æ€çš„è§„åˆ™ (æ‰€æœ‰ NPC å…±äº«è¿™éƒ¨åˆ†ç¼“å­˜)
        # 2. å†æ”¾æ¯ä¸ª NPC çš„ä¸ªæ€§è®¾å®š (system_prompt)
        # 3. æœ€åæ”¾å½“å‰ç¯å¢ƒæŠ¥å‘Š
        full_prompt = (
            f"{STATIC_PROMPT_PREFIX}"
            f"### è§’è‰²è®¾å®š\n{system_prompt}\n"
            f"### ç¯å¢ƒæŠ¥å‘Š\n{scene_report}\n"
            f"ä½ çš„å†³ç­–ï¼š"
        )
        # print('full_prompt',full_prompt)
        sampling_params = SamplingParams(
            temperature=body.get("temperature", 0.1),
            max_tokens=body.get("max_tokens", 2048),
            presence_penalty=0.3,
            stop=["<ï½œend of sentenceï½œ>", "###"]
        )

        # è¿è¡Œæ¨ç†
        outputs = llm.generate([full_prompt], sampling_params)
        raw_output = outputs[0].outputs[0].text

        # è§£æ JSON
        action_json = extract_json(raw_output)
        
        if action_json:
            # æå–æ€ç»´é“¾ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            think_match = re.search(r'<think>([\s\S]*?)</think>', raw_output)
            thinking_process = think_match.group(1).strip() if think_match else "æ— æ˜¾å¼æ€è€ƒè¿‡ç¨‹"

            return {
                "status": "success",
                "response": action_json,
                "thinking_raw": thinking_process
            }
        else:
            return {
                "status": "warning",
                "message": "æœªèƒ½è§£æå‡ºç¬¦åˆç»“æ„çš„ JSON",
                "raw_output": raw_output
            }

    except Exception as e:
        return {"status": "error", "message": str(e)}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
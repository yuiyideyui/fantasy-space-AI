from fastapi import FastAPI, Request
import uvicorn
import os
import re
import json
from vllm import LLM, SamplingParams

# å¼ºåˆ¶å…¼å®¹æ¨¡å¼ï¼ˆWSL / 4070Ti Super å‹å¥½ï¼‰
os.environ["VLLM_USE_V1"] = "0"

app = FastAPI()

# âœ… å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
llm = LLM(
    model="/home/yuiyi/models/DeepSeek-R1-14B-AWQ",
    trust_remote_code=True,
    max_model_len=4096,
    # ğŸ’¡ å…³é”®ä¿®æ”¹ç‚¹ 1ï¼šè°ƒé«˜åˆ° 0.9ã€‚16GB * 0.9 = 14.4GBï¼Œå‡å» 10GB æƒé‡ï¼Œè¿˜æœ‰ 4.4GB ç»™ Cacheï¼Œè¶³å¤Ÿäº†ã€‚
    gpu_memory_utilization=0.9, 
    # ğŸ’¡ å…³é”®ä¿®æ”¹ç‚¹ 2ï¼šæ˜¾å­˜åƒç´§æ—¶ï¼Œå¦‚æœä¸åšåˆ†å¸ƒå¼æ¨ç†ï¼Œä¸è¦å¼€è¿™ä¸ªï¼ˆé™¤éæ˜¯å•å¡å¤šå¹¶å‘å‡ºç°ç®—åŠ›ç“¶é¢ˆï¼‰
    enforce_eager=True, 
    # ğŸ’¡ å»ºè®®å¢åŠ ï¼š
    kv_cache_dtype="fp8", # å¦‚æœ vLLM ç‰ˆæœ¬æ”¯æŒï¼Œå¼€å¯ FP8 ç¼“å­˜èƒ½è®© Cache ç©ºé—´ç¿»å€ï¼Œæ”¯æŒæ›´å¤šå¹¶å‘
    disable_log_stats=True
)

@app.post("/generate")
async def generate(request: Request):
    body = await request.json()

    scene_report = body.get("scene_report")
    if not scene_report:
        return {"error": "scene_report is required"}

    system_prompt = body.get("system_prompt")
    if not system_prompt:
        return {"error": "system_prompt is required"}

    full_prompt = f"""
{system_prompt}
# ç¯å¢ƒæŠ¥å‘Š
{scene_report}
# ä»»åŠ¡
è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä½ çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
"""

    sampling_params = SamplingParams(
        temperature=body.get("temperature", 0.3),
        max_tokens=body.get("max_tokens", 512),
    )

    outputs = llm.generate([full_prompt], sampling_params)
    raw_text = outputs[0].outputs[0].text.strip()

    # ğŸ›¡ JSON è£å‰ªä¿é™©
    match = re.search(r"\{[\s\S]*\}", raw_text)
    if not match:
        return {"error": "Model did not return JSON", "raw": raw_text}

    json_text = match.group(0)

    try:
        parsed = json.loads(json_text)
    except json.JSONDecodeError:
        return {"error": "Invalid JSON", "raw": raw_text}

    return {"response": parsed}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
